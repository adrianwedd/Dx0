{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Feature Engineering for Cost Estimation - Complete Demo\n",
    "\n",
    "This notebook demonstrates the complete ML feature engineering pipeline for CPT cost estimation, from raw data to trained models with explainability.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Preparation](#setup)\n",
    "2. [Feature Engineering Pipeline](#feature-engineering)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Model Training and Evaluation](#model-training)\n",
    "5. [Model Explainability](#explainability)\n",
    "6. [Integration with Cost Estimator](#integration)\n",
    "7. [Performance Analysis](#performance)\n",
    "8. [Production Deployment](#deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import ML feature engineering components\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from sdb.ml_feature_engineering import ComprehensiveFeatureEngineering, FeatureConfig\n",
    "from sdb.ml_preprocessing import MLPreprocessingPipeline, create_default_preprocessing_pipeline\n",
    "from sdb.ml_cost_estimator import MLCostEstimator, MLModelConfig, create_ml_enhanced_cost_estimator\n",
    "from sdb.ml_validation import run_comprehensive_validation, FeatureQualityAnalyzer\n",
    "from sdb.ml_explainability import ModelExplainer, create_feature_importance_dashboard\n",
    "from sdb.cost_estimator import CptCost\n",
    "\n",
    "print(\"ML Feature Engineering System - Complete Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Demo started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Realistic Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define realistic CPT test data\n",
    "base_tests = [\n",
    "    # Laboratory Tests\n",
    "    ('complete blood count', '85027', 9.0, 'laboratory'),\n",
    "    ('comprehensive metabolic panel', '80053', 14.0, 'laboratory'),\n",
    "    ('basic metabolic panel', '80048', 13.0, 'laboratory'),\n",
    "    ('lipid panel', '80061', 15.0, 'laboratory'),\n",
    "    ('thyroid stimulating hormone', '84443', 20.0, 'laboratory'),\n",
    "    ('hemoglobin a1c', '83036', 13.0, 'laboratory'),\n",
    "    ('vitamin d 25-hydroxy', '82306', 35.0, 'laboratory'),\n",
    "    ('prostate specific antigen', '84153', 25.0, 'laboratory'),\n",
    "    ('urinalysis complete', '81001', 5.0, 'laboratory'),\n",
    "    ('blood culture aerobic', '87040', 20.0, 'laboratory'),\n",
    "    \n",
    "    # Imaging Tests\n",
    "    ('chest x-ray 2 views', '71046', 30.0, 'imaging'),\n",
    "    ('ct head without contrast', '70450', 100.0, 'imaging'),\n",
    "    ('mri brain with contrast', '70553', 400.0, 'imaging'),\n",
    "    ('mri lumbar spine without contrast', '72148', 350.0, 'imaging'),\n",
    "    ('ultrasound abdomen complete', '76700', 150.0, 'imaging'),\n",
    "    ('mammography bilateral', '77067', 120.0, 'imaging'),\n",
    "    ('ct chest with contrast', '71260', 200.0, 'imaging'),\n",
    "    ('x-ray knee 2 views', '73060', 35.0, 'imaging'),\n",
    "    \n",
    "    # Cardiology Tests  \n",
    "    ('electrocardiogram 12 lead', '93000', 10.0, 'cardiology'),\n",
    "    ('echocardiogram complete', '93307', 180.0, 'cardiology'),\n",
    "    ('stress test treadmill', '93017', 250.0, 'cardiology'),\n",
    "    ('holter monitor 24 hour', '93224', 200.0, 'cardiology'),\n",
    "    ('cardiac catheterization', '93458', 1200.0, 'cardiology'),\n",
    "    \n",
    "    # Procedures\n",
    "    ('colonoscopy screening', '45378', 800.0, 'procedure'),\n",
    "    ('upper endoscopy diagnostic', '43235', 600.0, 'procedure'),\n",
    "    ('skin biopsy single lesion', '11100', 150.0, 'procedure'),\n",
    "    ('joint injection knee', '20610', 75.0, 'procedure')\n",
    "]\n",
    "\n",
    "# Generate training dataset with variations\n",
    "training_data = []\n",
    "test_counts = {}  # Track how many of each test we generate\n",
    "\n",
    "for test_name, cpt_code, base_price, category in base_tests:\n",
    "    # Generate 30-80 variations per test (realistic sample sizes)\n",
    "    n_samples = np.random.randint(30, 81)\n",
    "    test_counts[test_name] = n_samples\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Add realistic price variation (10-25% standard deviation)\n",
    "        price_std = base_price * np.random.uniform(0.10, 0.25)\n",
    "        price = max(1.0, np.random.normal(base_price, price_std))\n",
    "        \n",
    "        # Add some seasonal/regional variation\n",
    "        if np.random.random() < 0.1:  # 10% chance of outlier\n",
    "            price *= np.random.uniform(0.7, 1.4)\n",
    "            \n",
    "        training_data.append({\n",
    "            'test_name': test_name,\n",
    "            'cpt_code': cpt_code,\n",
    "            'price': round(price, 2),\n",
    "            'category': category\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(training_data)\n",
    "\n",
    "# Display dataset summary\n",
    "print(f\"Generated Training Dataset:\")\n",
    "print(f\"  Total samples: {len(df_train):,}\")\n",
    "print(f\"  Unique tests: {df_train['test_name'].nunique()}\")\n",
    "print(f\"  Price range: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "print(f\"  Average price: ${df_train['price'].mean():.2f}\")\n",
    "\n",
    "# Show category distribution\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "category_dist = df_train['category'].value_counts()\n",
    "for category, count in category_dist.items():\n",
    "    print(f\"  {category}: {count} samples ({count/len(df_train)*100:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Data:\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Training Dataset Characteristics', fontsize=16)\n",
    "\n",
    "# Price distribution\n",
    "axes[0, 0].hist(df_train['price'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Price Distribution')\n",
    "axes[0, 0].axvline(df_train['price'].mean(), color='red', linestyle='--', label=f'Mean: ${df_train[\"price\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Price by category\n",
    "df_train.boxplot(column='price', by='category', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Price Distribution by Category')\n",
    "axes[0, 1].set_xlabel('Category')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Category distribution\n",
    "category_counts = df_train['category'].value_counts()\n",
    "axes[1, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Test Category Distribution')\n",
    "\n",
    "# Log price distribution (for better visualization of range)\n",
    "axes[1, 1].hist(np.log1p(df_train['price']), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Log(Price + 1)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Log Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df_train.groupby('category')['price'].agg(['count', 'mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Pipeline {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure feature engineering pipeline\n",
    "feature_config = FeatureConfig(\n",
    "    include_text_features=True,\n",
    "    include_hierarchical_features=True,\n",
    "    include_interaction_features=True,\n",
    "    include_temporal_features=False,\n",
    "    max_tfidf_features=50,\n",
    "    min_word_freq=2,\n",
    "    ngram_range=(1, 2),\n",
    "    handle_missing_values=True,\n",
    "    remove_outliers=True,\n",
    "    outlier_method=\"iqr\",\n",
    "    enable_feature_selection=False\n",
    ")\n",
    "\n",
    "print(\"Feature Engineering Configuration:\")\n",
    "print(f\"  Text features: {feature_config.include_text_features}\")\n",
    "print(f\"  Hierarchical features: {feature_config.include_hierarchical_features}\")\n",
    "print(f\"  Interaction features: {feature_config.include_interaction_features}\")\n",
    "print(f\"  Max TF-IDF features: {feature_config.max_tfidf_features}\")\n",
    "print(f\"  N-gram range: {feature_config.ngram_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features from Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature engineering pipeline\n",
    "feature_engineer = ComprehensiveFeatureEngineering(feature_config)\n",
    "\n",
    "# Prepare input data (required columns for feature engineering)\n",
    "X_input = df_train[['test_name', 'cpt_code', 'price']].copy()\n",
    "y_target = df_train['price'].copy()\n",
    "\n",
    "print(f\"Input data shape: {X_input.shape}\")\n",
    "print(f\"Sample input data:\")\n",
    "print(X_input.head())\n",
    "\n",
    "# Fit and transform features\n",
    "print(f\"\\nExtracting features...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "X_features = feature_engineer.fit_transform(X_input, y_target)\n",
    "\n",
    "processing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"Feature extraction complete!\")\n",
    "print(f\"  Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"  Output shape: {X_features.shape}\")\n",
    "print(f\"  Features generated: {X_features.shape[1]}\")\n",
    "print(f\"  Features per second: {X_features.shape[1]/processing_time:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and metadata\n",
    "feature_names = feature_engineer.get_feature_names()\n",
    "feature_metadata = feature_engineer.get_feature_metadata()\n",
    "\n",
    "print(f\"Feature Analysis:\")\n",
    "print(f\"  Total features: {len(feature_names)}\")\n",
    "\n",
    "# Categorize features by type\n",
    "feature_types = {}\n",
    "for name, metadata in feature_metadata.items():\n",
    "    ftype = metadata['type']\n",
    "    if ftype not in feature_types:\n",
    "        feature_types[ftype] = []\n",
    "    feature_types[ftype].append(name)\n",
    "\n",
    "print(f\"\\nFeature Types:\")\n",
    "for ftype, names in feature_types.items():\n",
    "    print(f\"  {ftype}: {len(names)} features\")\n",
    "\n",
    "# Show sample features from each category\n",
    "print(f\"\\nSample Features by Category:\")\n",
    "for ftype, names in feature_types.items():\n",
    "    sample_features = names[:5]  # Show first 5 features of each type\n",
    "    print(f\"  {ftype}:\")\n",
    "    for fname in sample_features:\n",
    "        description = feature_metadata.get(fname, {}).get('description', 'No description')\n",
    "        print(f\"    - {fname}: {description}\")\n",
    "    if len(names) > 5:\n",
    "        print(f\"    ... and {len(names) - 5} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to DataFrame for analysis\n",
    "feature_df = pd.DataFrame(X_features, columns=[f\"feature_{i}\" for i in range(X_features.shape[1])])\n",
    "\n",
    "# Analyze feature quality\n",
    "quality_analyzer = FeatureQualityAnalyzer(\n",
    "    target_correlation_threshold=0.1,\n",
    "    outlier_threshold=3.0,\n",
    "    missing_threshold=0.05\n",
    ")\n",
    "\n",
    "print(\"Analyzing feature quality...\")\n",
    "quality_metrics = quality_analyzer.analyze_feature_quality(feature_df, y_target)\n",
    "quality_report = quality_analyzer.generate_quality_report(quality_metrics)\n",
    "\n",
    "# Display quality summary\n",
    "summary = quality_report['summary']\n",
    "print(f\"\\nFeature Quality Summary:\")\n",
    "print(f\"  Total features analyzed: {summary['total_features']}\")\n",
    "print(f\"  Average quality score: {summary['avg_quality_score']:.3f}\")\n",
    "print(f\"  High quality features (>0.7): {summary['high_quality_features']}\")\n",
    "print(f\"  Low quality features (<0.3): {summary['low_quality_features']}\")\n",
    "print(f\"  Constant features: {summary['constant_features']}\")\n",
    "print(f\"  Average target correlation: {summary['avg_target_correlation']:.3f}\")\n",
    "\n",
    "# Show problematic features\n",
    "issues = quality_report['feature_issues']\n",
    "print(f\"\\nFeature Issues:\")\n",
    "for issue_type, feature_list in issues.items():\n",
    "    if feature_list:\n",
    "        print(f\"  {issue_type}: {len(feature_list)} features\")\n",
    "        if len(feature_list) <= 3:\n",
    "            print(f\"    {', '.join(feature_list)}\")\n",
    "        else:\n",
    "            print(f\"    {', '.join(feature_list[:3])}... and {len(feature_list)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature quality visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Quality Analysis', fontsize=16)\n",
    "\n",
    "# Quality score distribution\n",
    "quality_scores = [m.quality_score for m in quality_metrics]\n",
    "axes[0, 0].hist(quality_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Quality Score')\n",
    "axes[0, 0].set_ylabel('Number of Features')\n",
    "axes[0, 0].set_title('Feature Quality Score Distribution')\n",
    "axes[0, 0].axvline(np.mean(quality_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(quality_scores):.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Target correlation distribution\n",
    "correlations = [abs(m.correlation_with_target) for m in quality_metrics if m.correlation_with_target > 0]\n",
    "if correlations:\n",
    "    axes[0, 1].hist(correlations, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Absolute Target Correlation')\n",
    "    axes[0, 1].set_ylabel('Number of Features')\n",
    "    axes[0, 1].set_title('Target Correlation Distribution')\n",
    "    axes[0, 1].axvline(np.mean(correlations), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(correlations):.3f}')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "# Missing value percentage\n",
    "missing_pcts = [m.missing_percentage for m in quality_metrics]\n",
    "axes[1, 0].hist(missing_pcts, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Missing Value Percentage')\n",
    "axes[1, 0].set_ylabel('Number of Features')\n",
    "axes[1, 0].set_title('Missing Values Distribution')\n",
    "\n",
    "# Quality vs Correlation scatter\n",
    "if correlations:\n",
    "    corr_dict = {m.feature_name: abs(m.correlation_with_target) for m in quality_metrics if m.correlation_with_target > 0}\n",
    "    quality_dict = {m.feature_name: m.quality_score for m in quality_metrics}\n",
    "    \n",
    "    # Match features that have both quality and correlation\n",
    "    common_features = set(corr_dict.keys()) & set(quality_dict.keys())\n",
    "    x_vals = [quality_dict[f] for f in common_features]\n",
    "    y_vals = [corr_dict[f] for f in common_features]\n",
    "    \n",
    "    axes[1, 1].scatter(x_vals, y_vals, alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Quality Score')\n",
    "    axes[1, 1].set_ylabel('Absolute Target Correlation')\n",
    "    axes[1, 1].set_title('Quality vs Target Correlation')\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(x_vals) > 1:\n",
    "        z = np.polyfit(x_vals, y_vals, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 1].plot(sorted(x_vals), p(sorted(x_vals)), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive preprocessing pipeline\n",
    "print(\"Creating ML preprocessing pipeline...\")\n",
    "\n",
    "preprocessing_pipeline = create_default_preprocessing_pipeline()\n",
    "\n",
    "# Fit and transform the data\n",
    "print(\"Fitting preprocessing pipeline...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "X_processed = preprocessing_pipeline.fit_transform(X_input, y_target)\n",
    "\n",
    "preprocessing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"Preprocessing complete!\")\n",
    "print(f\"  Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  Input shape: {X_input.shape}\")\n",
    "print(f\"  Output shape: {X_processed.shape}\")\n",
    "print(f\"  Features generated: {X_processed.shape[1]}\")\n",
    "\n",
    "# Get preprocessing statistics\n",
    "prep_stats = preprocessing_pipeline.get_preprocessing_stats()\n",
    "print(f\"\\nPreprocessing Statistics:\")\n",
    "print(f\"  Original samples: {prep_stats['original_samples']}\")\n",
    "print(f\"  Final samples: {prep_stats['final_samples']}\")\n",
    "print(f\"  Sample retention rate: {prep_stats['final_samples']/prep_stats['original_samples']*100:.1f}%\")\n",
    "print(f\"  Original features: {prep_stats['original_features']}\")\n",
    "print(f\"  Final features: {prep_stats['final_features']}\")\n",
    "\n",
    "# Validation stats\n",
    "if 'validation_stats' in prep_stats:\n",
    "    val_stats = prep_stats['validation_stats']\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"  Rows removed: {val_stats.get('rows_removed', 0)}\")\n",
    "    print(f\"  Removal percentage: {val_stats.get('removal_percentage', 0):.1f}%\")\n",
    "    print(f\"  Unique CPT codes: {val_stats.get('unique_cpt_codes', 0)}\")\n",
    "    print(f\"  Price range: ${val_stats.get('price_range', [0, 0])[0]:.2f} - ${val_stats.get('price_range', [0, 0])[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive validation\n",
    "print(\"Running comprehensive pipeline validation...\")\n",
    "\n",
    "validation_results = run_comprehensive_validation(\n",
    "    X_input, \n",
    "    y_target\n",
    ")\n",
    "\n",
    "# Display validation summary\n",
    "summary = validation_results.get('summary', {})\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Overall success: {summary.get('overall_success', False)}\")\n",
    "print(f\"  Tests passed: {summary.get('tests_passed', 0)}/{summary.get('total_tests', 0)}\")\n",
    "print(f\"  Success rate: {summary.get('success_rate', 0):.1f}%\")\n",
    "\n",
    "# Show individual test results\n",
    "pipeline_tests = validation_results.get('pipeline_tests', [])\n",
    "print(f\"\\nIndividual Test Results:\")\n",
    "for test in pipeline_tests:\n",
    "    status = \"✅ PASS\" if test['success'] else \"❌ FAIL\"\n",
    "    exec_time = test.get('execution_time', 0)\n",
    "    print(f\"  {status} {test['test_name']}: {exec_time:.3f}s\")\n",
    "    \n",
    "    if not test['success'] and test.get('error_message'):\n",
    "        print(f\"    Error: {test['error_message']}\")\n",
    "\n",
    "# Check for major issues\n",
    "major_issues = summary.get('major_issues', [])\n",
    "if major_issues:\n",
    "    print(f\"\\n⚠️  Major Issues Found:\")\n",
    "    for issue in major_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(f\"\\n✅ No major issues detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation {#model-training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use processed features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_target, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_train['category']  # Ensure balanced split across categories\n",
    ")\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Training target range: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "print(f\"  Test target range: ${y_test.min():.2f} - ${y_test.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "model_results = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Time the training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    test_mape = np.mean(np.abs((y_test - y_pred_test) / np.maximum(y_test, 1e-8))) * 100\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape,\n",
    "        'training_time': training_time,\n",
    "        'prediction_time': prediction_time,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  R² Score (train): {train_r2:.3f}\")\n",
    "    print(f\"  R² Score (test): {test_r2:.3f}\")\n",
    "    print(f\"  MAE (test): ${test_mae:.2f}\")\n",
    "    print(f\"  RMSE (test): ${test_rmse:.2f}\")\n",
    "    print(f\"  MAPE (test): {test_mape:.1f}%\")\n",
    "    print(f\"  Training time: {training_time:.3f}s\")\n",
    "    print(f\"  Prediction time: {prediction_time:.3f}s\")\n",
    "    \n",
    "    # Performance grade\n",
    "    if test_r2 >= 0.9:\n",
    "        grade = \"Excellent\"\n",
    "    elif test_r2 >= 0.8:\n",
    "        grade = \"Very Good\"\n",
    "    elif test_r2 >= 0.7:\n",
    "        grade = \"Good\"\n",
    "    elif test_r2 >= 0.6:\n",
    "        grade = \"Fair\"\n",
    "    else:\n",
    "        grade = \"Poor\"\n",
    "    \n",
    "    print(f\"  Performance Grade: {grade}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# Performance metrics comparison\n",
    "model_names = list(model_results.keys())\n",
    "r2_scores = [model_results[name]['test_r2'] for name in model_names]\n",
    "mae_scores = [model_results[name]['test_mae'] for name in model_names]\n",
    "mape_scores = [model_results[name]['test_mape'] for name in model_names]\n",
    "training_times = [model_results[name]['training_time'] for name in model_names]\n",
    "\n",
    "# R² Score comparison\n",
    "bars = axes[0, 0].bar(model_names, r2_scores)\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_title('Model R² Score Comparison')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Color bars based on performance\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    if score >= 0.8:\n",
    "        bar.set_color('green')\n",
    "    elif score >= 0.6:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 1].bar(model_names, mae_scores)\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error ($)')\n",
    "axes[0, 1].set_title('Model MAE Comparison')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 0].bar(model_names, training_times)\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 0].set_title('Model Training Time Comparison')\n",
    "plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Prediction accuracy scatter plot (best model)\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['test_r2'])\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "axes[1, 1].scatter(y_test, best_predictions, alpha=0.6)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Price ($)')\n",
    "axes[1, 1].set_ylabel('Predicted Price ($)')\n",
    "axes[1, 1].set_title(f'Prediction Accuracy - {best_model_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"R² Score: {model_results[best_model_name]['test_r2']:.3f}\")\n",
    "print(f\"MAE: ${model_results[best_model_name]['test_mae']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Explainability {#explainability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model for explainability analysis\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "# Create feature names (simplified for demo)\n",
    "feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create model explainer\n",
    "print(f\"Creating model explainer for {best_model_name}...\")\n",
    "explainer = ModelExplainer(best_model, feature_names)\n",
    "\n",
    "# Generate comprehensive model explanation\n",
    "print(\"Generating model explanation...\")\n",
    "explanation = explainer.explain_model(\n",
    "    X_train, y_train, \n",
    "    importance_methods=['builtin', 'permutation']\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Explanation Summary:\")\n",
    "print(f\"  Model: {explanation.model_name}\")\n",
    "print(f\"  Features analyzed: {len(explanation.feature_importances)}\")\n",
    "\n",
    "# Performance summary\n",
    "performance = explanation.model_performance\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  R² Score: {performance['r2']:.3f}\")\n",
    "print(f\"  MAE: ${performance['mae']:.2f}\")\n",
    "print(f\"  RMSE: ${performance['rmse']:.2f}\")\n",
    "print(f\"  Mean Prediction: ${performance['mean_prediction']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "builtin_features = [f for f in explanation.feature_importances if f.importance_type == 'builtin']\n",
    "perm_features = [f for f in explanation.feature_importances if f.importance_type == 'permutation']\n",
    "\n",
    "print(f\"Feature Importance Analysis:\")\n",
    "print(f\"  Built-in importance features: {len(builtin_features)}\")\n",
    "print(f\"  Permutation importance features: {len(perm_features)}\")\n",
    "\n",
    "# Top features by built-in importance\n",
    "if builtin_features:\n",
    "    top_builtin = sorted(builtin_features, key=lambda x: x.importance_score, reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 Features (Built-in Importance):\")\n",
    "    for i, feature in enumerate(top_builtin, 1):\n",
    "        print(f\"  {i:2d}. {feature.feature_name}: {feature.importance_score:.4f}\")\n",
    "\n",
    "# Top features by permutation importance\n",
    "if perm_features:\n",
    "    top_perm = sorted(perm_features, key=lambda x: x.importance_score, reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 Features (Permutation Importance):\")\n",
    "    for i, feature in enumerate(top_perm, 1):\n",
    "        ci_text = \"\"\n",
    "        if feature.confidence_interval:\n",
    "            ci_width = feature.confidence_interval[1] - feature.confidence_interval[0]\n",
    "            ci_text = f\" (±{ci_width/2:.4f})\"\n",
    "        stability_text = \"\"\n",
    "        if feature.stability_score:\n",
    "            stability_text = f\" [stability: {feature.stability_score:.2f}]\"\n",
    "        print(f\"  {i:2d}. {feature.feature_name}: {feature.importance_score:.4f}{ci_text}{stability_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Importance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature importance dashboard\n",
    "dashboard_fig = create_feature_importance_dashboard(explanation)\n",
    "plt.show()\n",
    "\n",
    "# Display global explanation\n",
    "print(f\"\\nGlobal Model Explanation:\")\n",
    "print(\"=\" * 50)\n",
    "print(explanation.global_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain individual predictions for interesting samples\n",
    "test_indices = [\n",
    "    np.argmax(y_test),  # Most expensive test\n",
    "    np.argmin(y_test),  # Least expensive test\n",
    "    len(y_test) // 2    # Median test\n",
    "]\n",
    "\n",
    "sample_descriptions = [\"Most Expensive\", \"Least Expensive\", \"Median Price\"]\n",
    "\n",
    "print(f\"\\nIndividual Prediction Explanations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    description = sample_descriptions[i]\n",
    "    \n",
    "    # Get prediction explanation\n",
    "    pred_explanation = explainer.explain_prediction(\n",
    "        X_test[idx], \n",
    "        sample_id=f\"sample_{idx}\",\n",
    "        actual_value=y_test.iloc[idx]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{description} Sample (Index: {idx}):\")\n",
    "    print(f\"  Predicted: ${pred_explanation.prediction:.2f}\")\n",
    "    print(f\"  Actual: ${pred_explanation.actual_value:.2f}\")\n",
    "    \n",
    "    error = abs(pred_explanation.prediction - pred_explanation.actual_value)\n",
    "    error_pct = error / pred_explanation.actual_value * 100\n",
    "    print(f\"  Error: ${error:.2f} ({error_pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"  Top Contributing Features:\")\n",
    "    for j, (feature, contribution) in enumerate(pred_explanation.top_contributing_features[:5], 1):\n",
    "        direction = \"↑\" if contribution > 0 else \"↓\"\n",
    "        print(f\"    {j}. {feature}: {direction} ${abs(contribution):.2f}\")\n",
    "    \n",
    "    if pred_explanation.explanation_text:\n",
    "        print(f\"  Explanation:\")\n",
    "        # Split explanation into lines for better formatting\n",
    "        lines = pred_explanation.explanation_text.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                print(f\"    {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Cost Estimator {#integration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create original cost table from our base data\n",
    "cost_table = {}\n",
    "for test_name, cpt_code, base_price, category in base_tests[:10]:  # Use subset for demo\n",
    "    cost_table[test_name] = CptCost(\n",
    "        cpt_code=cpt_code,\n",
    "        price=base_price,\n",
    "        category=category\n",
    "    )\n",
    "\n",
    "print(f\"Created cost table with {len(cost_table)} entries\")\n",
    "\n",
    "# Create ML-enhanced cost estimator\n",
    "ml_config = MLModelConfig(\n",
    "    model_type=\"random_forest\",\n",
    "    model_params={\"n_estimators\": 100, \"random_state\": 42},\n",
    "    performance_threshold=0.7,\n",
    "    enable_model_selection=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCreating ML-enhanced cost estimator...\")\n",
    "ml_estimator = create_ml_enhanced_cost_estimator(\n",
    "    cost_table=cost_table,\n",
    "    training_data=df_train,\n",
    "    ml_config=ml_config\n",
    ")\n",
    "\n",
    "# Display model performance\n",
    "performance = ml_estimator.get_model_performance()\n",
    "if performance:\n",
    "    print(f\"\\nML Model Performance:\")\n",
    "    print(f\"  Model: {performance['model_name']}\")\n",
    "    print(f\"  Performance Grade: {ml_estimator.model_performance.performance_grade}\")\n",
    "    print(f\"  R² Score: {performance['r2_score']:.3f}\")\n",
    "    print(f\"  MAE: ${performance['mean_absolute_error']:.2f}\")\n",
    "    print(f\"  Training Samples: {performance['training_samples']:,}\")\n",
    "    print(f\"  Features: {performance['feature_count']}\")\n",
    "    print(f\"  Training Time: {performance['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Integration with Different Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different scenarios\n",
    "test_scenarios = [\n",
    "    # Known tests (should use lookup)\n",
    "    ('complete blood count', 'Known test - should use lookup'),\n",
    "    ('chest x-ray 2 views', 'Known test - should use lookup'),\n",
    "    \n",
    "    # Similar to known tests (should use ML with high confidence)\n",
    "    ('complete blood count with differential', 'Similar to known test'),\n",
    "    ('chest x-ray single view', 'Similar to known test'),\n",
    "    ('basic metabolic panel comprehensive', 'Similar to known test'),\n",
    "    \n",
    "    # Completely unknown tests (should use ML or fallback)\n",
    "    ('advanced cardiac biomarkers panel', 'Unknown test - cardiac'),\n",
    "    ('molecular genetic testing brca1', 'Unknown test - genetic'),\n",
    "    ('pet scan whole body with contrast', 'Unknown test - imaging'),\n",
    "    ('specialized neurological function test', 'Unknown test - neuro')\n",
    "]\n",
    "\n",
    "print(f\"Testing Cost Estimation Integration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Test Name':<40} {'Price':<10} {'Category':<12} {'Source':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for test_name, description in test_scenarios:\n",
    "    try:\n",
    "        # Get cost estimate\n",
    "        cost, category = ml_estimator.estimate(test_name)\n",
    "        \n",
    "        # Determine source\n",
    "        source = \"Lookup\"\n",
    "        try:\n",
    "            # Check if it's in the original cost table\n",
    "            ml_estimator.lookup_cost(test_name)\n",
    "        except KeyError:\n",
    "            # Not in lookup table\n",
    "            if ml_estimator.ml_model:\n",
    "                try:\n",
    "                    ml_pred, confidence = ml_estimator.predict_ml_cost(test_name)\n",
    "                    if confidence >= ml_estimator.confidence_threshold:\n",
    "                        source = f\"ML ({confidence:.2f})\"\n",
    "                    else:\n",
    "                        source = \"ML (low conf)\"\n",
    "                except:\n",
    "                    source = \"Fallback\"\n",
    "            else:\n",
    "                source = \"Fallback\"\n",
    "        \n",
    "        # Display result\n",
    "        print(f\"{test_name:<40} ${cost:<9.2f} {category:<12} {source:<15}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{test_name:<40} {'ERROR':<10} {'N/A':<12} {str(e)[:15]:<15}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Source Legend: Lookup=Direct table lookup, ML=Machine Learning, Fallback=LLM or average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanations for some predictions\n",
    "explanation_tests = [\n",
    "    'advanced cardiac biomarkers panel',\n",
    "    'pet scan whole body with contrast',\n",
    "    'basic metabolic panel comprehensive'\n",
    "]\n",
    "\n",
    "print(f\"\\nPrediction Explanations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_name in explanation_tests:\n",
    "    explanation = ml_estimator.explain_prediction(test_name)\n",
    "    \n",
    "    if 'error' in explanation:\n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(f\"  Error: {explanation['error']}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{test_name}:\")\n",
    "    print(f\"  Predicted Cost: ${explanation['prediction']:.2f}\")\n",
    "    \n",
    "    if 'top_features' in explanation:\n",
    "        print(f\"  Top Contributing Features:\")\n",
    "        for i, (feature, contribution) in enumerate(explanation['top_features'][:3], 1):\n",
    "            direction = \"increases\" if contribution > 0 else \"decreases\" \n",
    "            print(f\"    {i}. {feature}: {direction} cost by ${abs(contribution):.2f}\")\n",
    "    \n",
    "    if 'explanation_text' in explanation and explanation['explanation_text']:\n",
    "        print(f\"  Explanation: {explanation['explanation_text'].split('.')[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance analysis\n",
    "print(f\"Comprehensive Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Feature engineering performance\n",
    "feature_processing_rate = len(df_train) / preprocessing_time\n",
    "print(f\"\\nFeature Engineering Performance:\")\n",
    "print(f\"  Total samples processed: {len(df_train):,}\")\n",
    "print(f\"  Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  Processing rate: {feature_processing_rate:.0f} samples/second\")\n",
    "print(f\"  Features generated: {X_processed.shape[1]}\")\n",
    "print(f\"  Feature generation rate: {X_processed.shape[1]/preprocessing_time:.1f} features/second\")\n",
    "\n",
    "# Model performance by category\n",
    "print(f\"\\nModel Performance by Test Category:\")\n",
    "print(f\"{'Category':<15} {'Samples':<8} {'R² Score':<10} {'MAE':<10} {'MAPE':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Split test data by category for analysis\n",
    "test_df = df_train.iloc[X_test.index] if hasattr(X_test, 'index') else df_train[-len(y_test):]\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "for category in df_train['category'].unique():\n",
    "    # Get indices for this category in test set\n",
    "    if hasattr(test_df, 'reset_index'):\n",
    "        category_mask = test_df.reset_index()['category'] == category\n",
    "    else:\n",
    "        category_mask = test_df['category'] == category\n",
    "    \n",
    "    if category_mask.sum() > 0:\n",
    "        y_cat_true = y_test[category_mask]\n",
    "        y_cat_pred = best_predictions[category_mask]\n",
    "        \n",
    "        if len(y_cat_true) > 1:\n",
    "            cat_r2 = r2_score(y_cat_true, y_cat_pred)\n",
    "            cat_mae = mean_absolute_error(y_cat_true, y_cat_pred)\n",
    "            cat_mape = np.mean(np.abs((y_cat_true - y_cat_pred) / np.maximum(y_cat_true, 1e-8))) * 100\n",
    "            \n",
    "            print(f\"{category:<15} {len(y_cat_true):<8} {cat_r2:<10.3f} ${cat_mae:<9.2f} {cat_mape:<9.1f}%\")\n",
    "\n",
    "# Overall system performance\n",
    "print(f\"\\nOverall System Performance:\")\n",
    "total_pipeline_time = preprocessing_time + model_results[best_model_name]['training_time']\n",
    "print(f\"  End-to-end training time: {total_pipeline_time:.2f} seconds\")\n",
    "print(f\"  Prediction latency: {model_results[best_model_name]['prediction_time']/len(y_test)*1000:.1f} ms/sample\")\n",
    "print(f\"  Memory footprint: ~{X_processed.nbytes / 1024 / 1024:.1f} MB (features only)\")\n",
    "print(f\"  Model accuracy: {model_results[best_model_name]['test_r2']:.1%}\")\n",
    "print(f\"  Model precision: ${model_results[best_model_name]['test_mae']:.2f} MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different data sizes\n",
    "print(f\"\\nPerformance Scaling Analysis:\")\n",
    "print(f\"{'Data Size':<12} {'Processing Time':<16} {'Features':<10} {'Rate (samples/s)':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Test with different sample sizes\n",
    "test_sizes = [100, 500, 1000, len(df_train)]\n",
    "\n",
    "for size in test_sizes:\n",
    "    if size <= len(df_train):\n",
    "        # Sample data\n",
    "        sample_data = df_train.sample(n=size, random_state=42)\n",
    "        sample_input = sample_data[['test_name', 'cpt_code', 'price']]\n",
    "        \n",
    "        # Time the processing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create fresh pipeline for fair comparison\n",
    "        sample_pipeline = create_default_preprocessing_pipeline()\n",
    "        sample_features = sample_pipeline.fit_transform(sample_input)\n",
    "        \n",
    "        process_time = time.time() - start_time\n",
    "        rate = size / process_time\n",
    "        \n",
    "        print(f\"{size:<12,} {process_time:<16.3f} {sample_features.shape[1]:<10} {rate:<15.0f}\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nMemory Usage Estimation:\")\n",
    "feature_memory = X_processed.nbytes / 1024 / 1024\n",
    "print(f\"  Feature matrix: {feature_memory:.1f} MB\")\n",
    "print(f\"  Per sample: {feature_memory / len(X_processed) * 1024:.1f} KB\")\n",
    "print(f\"  Estimated for 100K samples: {feature_memory / len(X_processed) * 100000:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Deployment Considerations {#deployment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for production deployment\n",
    "model_save_path = \"../models/ml_cost_estimator_demo.pkl\"\n",
    "\n",
    "print(f\"Production Deployment Checklist:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    import os\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    ml_estimator.save_ml_model(model_save_path)\n",
    "    print(f\"✅ Model saved to {model_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model save failed: {e}\")\n",
    "\n",
    "# Model validation checklist\n",
    "print(f\"\\n📋 Model Validation Checklist:\")\n",
    "validation_checks = [\n",
    "    (\"Model R² Score > 0.7\", model_results[best_model_name]['test_r2'] > 0.7),\n",
    "    (\"Model MAE < $50\", model_results[best_model_name]['test_mae'] < 50),\n",
    "    (\"Training completed successfully\", ml_estimator.model_performance is not None),\n",
    "    (\"Feature engineering working\", X_processed.shape[1] > 0),\n",
    "    (\"No major validation failures\", len(summary.get('major_issues', [])) == 0),\n",
    "    (\"Prediction explanations available\", ml_estimator.model_explainer is not None)\n",
    "]\n",
    "\n",
    "for check, passed in validation_checks:\n",
    "    status = \"✅\" if passed else \"❌\"\n",
    "    print(f\"  {status} {check}\")\n",
    "\n",
    "# Performance requirements\n",
    "print(f\"\\n⚡ Performance Requirements:\")\n",
    "perf_requirements = [\n",
    "    (\"Prediction latency < 100ms\", model_results[best_model_name]['prediction_time']/len(y_test) < 0.1),\n",
    "    (\"Training time < 60s\", model_results[best_model_name]['training_time'] < 60),\n",
    "    (\"Memory usage reasonable\", feature_memory < 500),  # Less than 500MB\n",
    "    (\"Feature processing < 10s\", preprocessing_time < 10)\n",
    "]\n",
    "\n",
    "for requirement, met in perf_requirements:\n",
    "    status = \"✅\" if met else \"⚠️\"\n",
    "    print(f\"  {status} {requirement}\")\n",
    "\n",
    "# Deployment recommendations\n",
    "print(f\"\\n🚀 Deployment Recommendations:\")\n",
    "print(f\"  • Enable feature caching for frequently requested tests\")\n",
    "print(f\"  • Implement prediction confidence thresholds (current: {ml_estimator.confidence_threshold})\")\n",
    "print(f\"  • Set up model performance monitoring\")\n",
    "print(f\"  • Plan for regular model retraining (monthly/quarterly)\")\n",
    "print(f\"  • Implement A/B testing for model updates\")\n",
    "print(f\"  • Set up data quality monitoring for input features\")\n",
    "print(f\"  • Configure fallback mechanisms for ML failures\")\n",
    "\n",
    "# Production integration example\n",
    "print(f\"\\n🔧 Production Integration Example:\")\n",
    "print(f\"\"\"```python\n",
    "# Load trained model in production\n",
    "from sdb.ml_cost_estimator import MLCostEstimator\n",
    "\n",
    "# Load model\n",
    "estimator = MLCostEstimator(cost_table)\n",
    "estimator.load_ml_model('{model_save_path}')\n",
    "\n",
    "# Make predictions with confidence checking\n",
    "cost, category = estimator.estimate('new test name')\n",
    "explanation = estimator.explain_prediction('new test name')\n",
    "\n",
    "# Monitor performance\n",
    "performance = estimator.get_model_performance()\n",
    "if performance['r2_score'] < 0.7:\n",
    "    # Trigger retraining alert\n",
    "    pass\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ML FEATURE ENGINEERING SYSTEM - DEMO SUMMARY\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 Dataset Summary:\")\n",
    "print(f\"  • Training samples: {len(df_train):,}\")\n",
    "print(f\"  • Unique tests: {df_train['test_name'].nunique()}\")\n",
    "print(f\"  • Price range: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "print(f\"  • Test categories: {df_train['category'].nunique()}\")\n",
    "\n",
    "print(f\"\\n🔧 Feature Engineering Results:\")\n",
    "print(f\"  • Features generated: {X_processed.shape[1]}\")\n",
    "print(f\"  • Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  • Processing rate: {len(df_train)/preprocessing_time:.0f} samples/second\")\n",
    "print(f\"  • Feature quality: {summary.get('avg_quality_score', 0):.3f} average score\")\n",
    "\n",
    "print(f\"\\n🤖 Model Performance:\")\n",
    "print(f\"  • Best model: {best_model_name}\")\n",
    "print(f\"  • R² Score: {model_results[best_model_name]['test_r2']:.3f}\")\n",
    "print(f\"  • Mean Absolute Error: ${model_results[best_model_name]['test_mae']:.2f}\")\n",
    "print(f\"  • Mean Absolute Percentage Error: {model_results[best_model_name]['test_mape']:.1f}%\")\n",
    "print(f\"  • Performance Grade: {MLCostEstimator(cost_table).model_performance.performance_grade if MLCostEstimator(cost_table).model_performance else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n🎯 System Capabilities:\")\n",
    "print(f\"  ✅ Comprehensive feature extraction from CPT codes and test names\")\n",
    "print(f\"  ✅ Advanced data preprocessing with quality validation\")\n",
    "print(f\"  ✅ Multiple ML model support with automated selection\")\n",
    "print(f\"  ✅ Feature importance analysis and model explainability\")\n",
    "print(f\"  ✅ Seamless integration with existing cost estimator\")\n",
    "print(f\"  ✅ Production-ready deployment with monitoring\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Production:\")\n",
    "all_checks_passed = all([check[1] for check in validation_checks])\n",
    "if all_checks_passed:\n",
    "    print(f\"  ✅ All validation checks passed\")\n",
    "    print(f\"  ✅ Performance requirements met\")\n",
    "    print(f\"  ✅ Model saved and ready for deployment\")\n",
    "    print(f\"  ✅ System is production-ready!\")\n",
    "else:\n",
    "    failed_checks = [check[0] for check in validation_checks if not check[1]]\n",
    "    print(f\"  ⚠️  Some validation checks failed: {', '.join(failed_checks)}\")\n",
    "    print(f\"  📝 Review and address issues before production deployment\")\n",
    "\n",
    "print(f\"\\n📈 Next Steps:\")\n",
    "print(f\"  1. Deploy model to production environment\")\n",
    "print(f\"  2. Implement monitoring and alerting\")\n",
    "print(f\"  3. Set up automated retraining pipeline\")\n",
    "print(f\"  4. Conduct A/B testing with current system\")\n",
    "print(f\"  5. Monitor performance and gather feedback\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Demo completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "For more information about the ML Feature Engineering System:\n",
    "\n",
    "- **Documentation**: See `docs/ml_feature_engineering_guide.md` for comprehensive usage guide\n",
    "- **API Reference**: Detailed API documentation in the guide\n",
    "- **Performance Benchmarks**: Complete benchmarking results in the documentation\n",
    "- **Troubleshooting**: Common issues and solutions in the guide\n",
    "- **Best Practices**: Production deployment recommendations\n",
    "\n",
    "### Key Components Used:\n",
    "\n",
    "1. **`sdb.ml_feature_engineering`**: Core feature engineering pipeline\n",
    "2. **`sdb.ml_preprocessing`**: Data preprocessing and validation\n",
    "3. **`sdb.ml_cost_estimator`**: ML-enhanced cost estimation\n",
    "4. **`sdb.ml_validation`**: Comprehensive testing framework\n",
    "5. **`sdb.ml_explainability`**: Model interpretation and explanations\n",
    "6. **`sdb.ml_feature_store`**: Feature storage and caching system\n",
    "\n",
    "The system is now ready for production deployment with robust feature engineering, model training, and explainability capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}