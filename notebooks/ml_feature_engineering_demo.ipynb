{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Feature Engineering for Cost Estimation - Complete Demo\n",
    "\n",
    "This notebook demonstrates the complete ML feature engineering pipeline for CPT cost estimation, from raw data to trained models with explainability.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Preparation](#setup)\n",
    "2. [Feature Engineering Pipeline](#feature-engineering)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Model Training and Evaluation](#model-training)\n",
    "5. [Model Explainability](#explainability)\n",
    "6. [Integration with Cost Estimator](#integration)\n",
    "7. [Performance Analysis](#performance)\n",
    "8. [Production Deployment](#deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import ML feature engineering components\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from sdb.ml_feature_engineering import ComprehensiveFeatureEngineering, FeatureConfig\n",
    "from sdb.ml_preprocessing import MLPreprocessingPipeline, create_default_preprocessing_pipeline\n",
    "from sdb.ml_cost_estimator import MLCostEstimator, MLModelConfig, create_ml_enhanced_cost_estimator\n",
    "from sdb.ml_validation import run_comprehensive_validation, FeatureQualityAnalyzer\n",
    "from sdb.ml_explainability import ModelExplainer, create_feature_importance_dashboard\n",
    "from sdb.cost_estimator import CptCost\n",
    "\n",
    "print(\"ML Feature Engineering System - Complete Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Demo started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Realistic Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define realistic CPT test data\n",
    "base_tests = [\n",
    "    # Laboratory Tests\n",
    "    ('complete blood count', '85027', 9.0, 'laboratory'),\n",
    "    ('comprehensive metabolic panel', '80053', 14.0, 'laboratory'),\n",
    "    ('basic metabolic panel', '80048', 13.0, 'laboratory'),\n",
    "    ('lipid panel', '80061', 15.0, 'laboratory'),\n",
    "    ('thyroid stimulating hormone', '84443', 20.0, 'laboratory'),\n",
    "    ('hemoglobin a1c', '83036', 13.0, 'laboratory'),\n",
    "    ('vitamin d 25-hydroxy', '82306', 35.0, 'laboratory'),\n",
    "    ('prostate specific antigen', '84153', 25.0, 'laboratory'),\n",
    "    ('urinalysis complete', '81001', 5.0, 'laboratory'),\n",
    "    ('blood culture aerobic', '87040', 20.0, 'laboratory'),\n",
    "    \n",
    "    # Imaging Tests\n",
    "    ('chest x-ray 2 views', '71046', 30.0, 'imaging'),\n",
    "    ('ct head without contrast', '70450', 100.0, 'imaging'),\n",
    "    ('mri brain with contrast', '70553', 400.0, 'imaging'),\n",
    "    ('mri lumbar spine without contrast', '72148', 350.0, 'imaging'),\n",
    "    ('ultrasound abdomen complete', '76700', 150.0, 'imaging'),\n",
    "    ('mammography bilateral', '77067', 120.0, 'imaging'),\n",
    "    ('ct chest with contrast', '71260', 200.0, 'imaging'),\n",
    "    ('x-ray knee 2 views', '73060', 35.0, 'imaging'),\n",
    "    \n",
    "    # Cardiology Tests  \n",
    "    ('electrocardiogram 12 lead', '93000', 10.0, 'cardiology'),\n",
    "    ('echocardiogram complete', '93307', 180.0, 'cardiology'),\n",
    "    ('stress test treadmill', '93017', 250.0, 'cardiology'),\n",
    "    ('holter monitor 24 hour', '93224', 200.0, 'cardiology'),\n",
    "    ('cardiac catheterization', '93458', 1200.0, 'cardiology'),\n",
    "    \n",
    "    # Procedures\n",
    "    ('colonoscopy screening', '45378', 800.0, 'procedure'),\n",
    "    ('upper endoscopy diagnostic', '43235', 600.0, 'procedure'),\n",
    "    ('skin biopsy single lesion', '11100', 150.0, 'procedure'),\n",
    "    ('joint injection knee', '20610', 75.0, 'procedure')\n",
    "]\n",
    "\n",
    "# Generate training dataset with variations\n",
    "training_data = []\n",
    "test_counts = {}  # Track how many of each test we generate\n",
    "\n",
    "for test_name, cpt_code, base_price, category in base_tests:\n",
    "    # Generate 30-80 variations per test (realistic sample sizes)\n",
    "    n_samples = np.random.randint(30, 81)\n",
    "    test_counts[test_name] = n_samples\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Add realistic price variation (10-25% standard deviation)\n",
    "        price_std = base_price * np.random.uniform(0.10, 0.25)\n",
    "        price = max(1.0, np.random.normal(base_price, price_std))\n",
    "        \n",
    "        # Add some seasonal/regional variation\n",
    "        if np.random.random() < 0.1:  # 10% chance of outlier\n",
    "            price *= np.random.uniform(0.7, 1.4)\n",
    "            \n",
    "        training_data.append({\n",
    "            'test_name': test_name,\n",
    "            'cpt_code': cpt_code,\n",
    "            'price': round(price, 2),\n",
    "            'category': category\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(training_data)\n",
    "\n",
    "# Display dataset summary\n",
    "print(f\"Generated Training Dataset:\")\n",
    "print(f\"  Total samples: {len(df_train):,}\")\n",
    "print(f\"  Unique tests: {df_train['test_name'].nunique()}\")\n",
    "print(f\"  Price range: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "print(f\"  Average price: ${df_train['price'].mean():.2f}\")\n",
    "\n",
    "# Show category distribution\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "category_dist = df_train['category'].value_counts()\n",
    "for category, count in category_dist.items():\n",
    "    print(f\"  {category}: {count} samples ({count/len(df_train)*100:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Data:\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Training Dataset Characteristics', fontsize=16)\n",
    "\n",
    "# Price distribution\n",
    "axes[0, 0].hist(df_train['price'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Price Distribution')\n",
    "axes[0, 0].axvline(df_train['price'].mean(), color='red', linestyle='--', label=f'Mean: ${df_train[\"price\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Price by category\n",
    "df_train.boxplot(column='price', by='category', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Price Distribution by Category')\n",
    "axes[0, 1].set_xlabel('Category')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Category distribution\n",
    "category_counts = df_train['category'].value_counts()\n",
    "axes[1, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Test Category Distribution')\n",
    "\n",
    "# Log price distribution (for better visualization of range)\n",
    "axes[1, 1].hist(np.log1p(df_train['price']), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Log(Price + 1)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Log Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df_train.groupby('category')['price'].agg(['count', 'mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Pipeline {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure feature engineering pipeline\n",
    "feature_config = FeatureConfig(\n",
    "    include_text_features=True,\n",
    "    include_hierarchical_features=True,\n",
    "    include_interaction_features=True,\n",
    "    include_temporal_features=False,\n",
    "    max_tfidf_features=50,\n",
    "    min_word_freq=2,\n",
    "    ngram_range=(1, 2),\n",
    "    handle_missing_values=True,\n",
    "    remove_outliers=True,\n",
    "    outlier_method=\"iqr\",\n",
    "    enable_feature_selection=False\n",
    ")\n",
    "\n",
    "print(\"Feature Engineering Configuration:\")\n",
    "print(f\"  Text features: {feature_config.include_text_features}\")\n",
    "print(f\"  Hierarchical features: {feature_config.include_hierarchical_features}\")\n",
    "print(f\"  Interaction features: {feature_config.include_interaction_features}\")\n",
    "print(f\"  Max TF-IDF features: {feature_config.max_tfidf_features}\")\n",
    "print(f\"  N-gram range: {feature_config.ngram_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features from Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature engineering pipeline\n",
    "feature_engineer = ComprehensiveFeatureEngineering(feature_config)\n",
    "\n",
    "# Prepare input data (required columns for feature engineering)\n",
    "X_input = df_train[['test_name', 'cpt_code', 'price']].copy()\n",
    "y_target = df_train['price'].copy()\n",
    "\n",
    "print(f\"Input data shape: {X_input.shape}\")\n",
    "print(f\"Sample input data:\")\n",
    "print(X_input.head())\n",
    "\n",
    "# Fit and transform features\n",
    "print(f\"\\nExtracting features...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "X_features = feature_engineer.fit_transform(X_input, y_target)\n",
    "\n",
    "processing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"Feature extraction complete!\")\n",
    "print(f\"  Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"  Output shape: {X_features.shape}\")\n",
    "print(f\"  Features generated: {X_features.shape[1]}\")\n",
    "print(f\"  Features per second: {X_features.shape[1]/processing_time:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and metadata\n",
    "feature_names = feature_engineer.get_feature_names()\n",
    "feature_metadata = feature_engineer.get_feature_metadata()\n",
    "\n",
    "print(f\"Feature Analysis:\")\n",
    "print(f\"  Total features: {len(feature_names)}\")\n",
    "\n",
    "# Categorize features by type\n",
    "feature_types = {}\n",
    "for name, metadata in feature_metadata.items():\n",
    "    ftype = metadata['type']\n",
    "    if ftype not in feature_types:\n",
    "        feature_types[ftype] = []\n",
    "    feature_types[ftype].append(name)\n",
    "\n",
    "print(f\"\\nFeature Types:\")\n",
    "for ftype, names in feature_types.items():\n",
    "    print(f\"  {ftype}: {len(names)} features\")\n",
    "\n",
    "# Show sample features from each category\n",
    "print(f\"\\nSample Features by Category:\")\n",
    "for ftype, names in feature_types.items():\n",
    "    sample_features = names[:5]  # Show first 5 features of each type\n",
    "    print(f\"  {ftype}:\")\n",
    "    for fname in sample_features:\n",
    "        description = feature_metadata.get(fname, {}).get('description', 'No description')\n",
    "        print(f\"    - {fname}: {description}\")\n",
    "    if len(names) > 5:\n",
    "        print(f\"    ... and {len(names) - 5} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to DataFrame for analysis\n",
    "feature_df = pd.DataFrame(X_features, columns=[f\"feature_{i}\" for i in range(X_features.shape[1])])\n",
    "\n",
    "# Analyze feature quality\n",
    "quality_analyzer = FeatureQualityAnalyzer(\n",
    "    target_correlation_threshold=0.1,\n",
    "    outlier_threshold=3.0,\n",
    "    missing_threshold=0.05\n",
    ")\n",
    "\n",
    "print(\"Analyzing feature quality...\")\n",
    "quality_metrics = quality_analyzer.analyze_feature_quality(feature_df, y_target)\n",
    "quality_report = quality_analyzer.generate_quality_report(quality_metrics)\n",
    "\n",
    "# Display quality summary\n",
    "summary = quality_report['summary']\n",
    "print(f\"\\nFeature Quality Summary:\")\n",
    "print(f\"  Total features analyzed: {summary['total_features']}\")\n",
    "print(f\"  Average quality score: {summary['avg_quality_score']:.3f}\")\n",
    "print(f\"  High quality features (>0.7): {summary['high_quality_features']}\")\n",
    "print(f\"  Low quality features (<0.3): {summary['low_quality_features']}\")\n",
    "print(f\"  Constant features: {summary['constant_features']}\")\n",
    "print(f\"  Average target correlation: {summary['avg_target_correlation']:.3f}\")\n",
    "\n",
    "# Show problematic features\n",
    "issues = quality_report['feature_issues']\n",
    "print(f\"\\nFeature Issues:\")\n",
    "for issue_type, feature_list in issues.items():\n",
    "    if feature_list:\n",
    "        print(f\"  {issue_type}: {len(feature_list)} features\")\n",
    "        if len(feature_list) <= 3:\n",
    "            print(f\"    {', '.join(feature_list)}\")\n",
    "        else:\n",
    "            print(f\"    {', '.join(feature_list[:3])}... and {len(feature_list)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature quality visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Quality Analysis', fontsize=16)\n",
    "\n",
    "# Quality score distribution\n",
    "quality_scores = [m.quality_score for m in quality_metrics]\n",
    "axes[0, 0].hist(quality_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Quality Score')\n",
    "axes[0, 0].set_ylabel('Number of Features')\n",
    "axes[0, 0].set_title('Feature Quality Score Distribution')\n",
    "axes[0, 0].axvline(np.mean(quality_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(quality_scores):.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Target correlation distribution\n",
    "correlations = [abs(m.correlation_with_target) for m in quality_metrics if m.correlation_with_target > 0]\n",
    "if correlations:\n",
    "    axes[0, 1].hist(correlations, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Absolute Target Correlation')\n",
    "    axes[0, 1].set_ylabel('Number of Features')\n",
    "    axes[0, 1].set_title('Target Correlation Distribution')\n",
    "    axes[0, 1].axvline(np.mean(correlations), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(correlations):.3f}')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "# Missing value percentage\n",
    "missing_pcts = [m.missing_percentage for m in quality_metrics]\n",
    "axes[1, 0].hist(missing_pcts, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Missing Value Percentage')\n",
    "axes[1, 0].set_ylabel('Number of Features')\n",
    "axes[1, 0].set_title('Missing Values Distribution')\n",
    "\n",
    "# Quality vs Correlation scatter\n",
    "if correlations:\n",
    "    corr_dict = {m.feature_name: abs(m.correlation_with_target) for m in quality_metrics if m.correlation_with_target > 0}\n",
    "    quality_dict = {m.feature_name: m.quality_score for m in quality_metrics}\n",
    "    \n",
    "    # Match features that have both quality and correlation\n",
    "    common_features = set(corr_dict.keys()) & set(quality_dict.keys())\n",
    "    x_vals = [quality_dict[f] for f in common_features]\n",
    "    y_vals = [corr_dict[f] for f in common_features]\n",
    "    \n",
    "    axes[1, 1].scatter(x_vals, y_vals, alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Quality Score')\n",
    "    axes[1, 1].set_ylabel('Absolute Target Correlation')\n",
    "    axes[1, 1].set_title('Quality vs Target Correlation')\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(x_vals) > 1:\n",
    "        z = np.polyfit(x_vals, y_vals, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 1].plot(sorted(x_vals), p(sorted(x_vals)), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive preprocessing pipeline\n",
    "print(\"Creating ML preprocessing pipeline...\")\n",
    "\n",
    "preprocessing_pipeline = create_default_preprocessing_pipeline()\n",
    "\n",
    "# Fit and transform the data\n",
    "print(\"Fitting preprocessing pipeline...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "X_processed = preprocessing_pipeline.fit_transform(X_input, y_target)\n",
    "\n",
    "preprocessing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"Preprocessing complete!\")\n",
    "print(f\"  Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  Input shape: {X_input.shape}\")\n",
    "print(f\"  Output shape: {X_processed.shape}\")\n",
    "print(f\"  Features generated: {X_processed.shape[1]}\")\n",
    "\n",
    "# Get preprocessing statistics\n",
    "prep_stats = preprocessing_pipeline.get_preprocessing_stats()\n",
    "print(f\"\\nPreprocessing Statistics:\")\n",
    "print(f\"  Original samples: {prep_stats['original_samples']}\")\n",
    "print(f\"  Final samples: {prep_stats['final_samples']}\")\n",
    "print(f\"  Sample retention rate: {prep_stats['final_samples']/prep_stats['original_samples']*100:.1f}%\")\n",
    "print(f\"  Original features: {prep_stats['original_features']}\")\n",
    "print(f\"  Final features: {prep_stats['final_features']}\")\n",
    "\n",
    "# Validation stats\n",
    "if 'validation_stats' in prep_stats:\n",
    "    val_stats = prep_stats['validation_stats']\n",
    "    print(f\"\\nData Validation:\")\n",
    "    print(f\"  Rows removed: {val_stats.get('rows_removed', 0)}\")\n",
    "    print(f\"  Removal percentage: {val_stats.get('removal_percentage', 0):.1f}%\")\n",
    "    print(f\"  Unique CPT codes: {val_stats.get('unique_cpt_codes', 0)}\")\n",
    "    print(f\"  Price range: ${val_stats.get('price_range', [0, 0])[0]:.2f} - ${val_stats.get('price_range', [0, 0])[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive validation\n",
    "print(\"Running comprehensive pipeline validation...\")\n",
    "\n",
    "validation_results = run_comprehensive_validation(\n",
    "    X_input, \n",
    "    y_target\n",
    ")\n",
    "\n",
    "# Display validation summary\n",
    "summary = validation_results.get('summary', {})\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Overall success: {summary.get('overall_success', False)}\")\n",
    "print(f\"  Tests passed: {summary.get('tests_passed', 0)}/{summary.get('total_tests', 0)}\")\n",
    "print(f\"  Success rate: {summary.get('success_rate', 0):.1f}%\")\n",
    "\n",
    "# Show individual test results\n",
    "pipeline_tests = validation_results.get('pipeline_tests', [])\n",
    "print(f\"\\nIndividual Test Results:\")\n",
    "for test in pipeline_tests:\n",
    "    status = \"‚úÖ PASS\" if test['success'] else \"‚ùå FAIL\"\n",
    "    exec_time = test.get('execution_time', 0)\n",
    "    print(f\"  {status} {test['test_name']}: {exec_time:.3f}s\")\n",
    "    \n",
    "    if not test['success'] and test.get('error_message'):\n",
    "        print(f\"    Error: {test['error_message']}\")\n",
    "\n",
    "# Check for major issues\n",
    "major_issues = summary.get('major_issues', [])\n",
    "if major_issues:\n",
    "    print(f\"\\n‚ö†Ô∏è  Major Issues Found:\")\n",
    "    for issue in major_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No major issues detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation {#model-training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use processed features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_target, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_train['category']  # Ensure balanced split across categories\n",
    ")\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Training target range: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "print(f\"  Test target range: ${y_test.min():.2f} - ${y_test.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "model_results = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Time the training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    test_mape = np.mean(np.abs((y_test - y_pred_test) / np.maximum(y_test, 1e-8))) * 100\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape,\n",
    "        'training_time': training_time,\n",
    "        'prediction_time': prediction_time,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  R¬≤ Score (train): {train_r2:.3f}\")\n",
    "    print(f\"  R¬≤ Score (test): {test_r2:.3f}\")\n",
    "    print(f\"  MAE (test): ${test_mae:.2f}\")\n",
    "    print(f\"  RMSE (test): ${test_rmse:.2f}\")\n",
    "    print(f\"  MAPE (test): {test_mape:.1f}%\")\n",
    "    print(f\"  Training time: {training_time:.3f}s\")\n",
    "    print(f\"  Prediction time: {prediction_time:.3f}s\")\n",
    "    \n",
    "    # Performance grade\n",
    "    if test_r2 >= 0.9:\n",
    "        grade = \"Excellent\"\n",
    "    elif test_r2 >= 0.8:\n",
    "        grade = \"Very Good\"\n",
    "    elif test_r2 >= 0.7:\n",
    "        grade = \"Good\"\n",
    "    elif test_r2 >= 0.6:\n",
    "        grade = \"Fair\"\n",
    "    else:\n",
    "        grade = \"Poor\"\n",
    "    \n",
    "    print(f\"  Performance Grade: {grade}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# Performance metrics comparison\n",
    "model_names = list(model_results.keys())\n",
    "r2_scores = [model_results[name]['test_r2'] for name in model_names]\n",
    "mae_scores = [model_results[name]['test_mae'] for name in model_names]\n",
    "mape_scores = [model_results[name]['test_mape'] for name in model_names]\n",
    "training_times = [model_results[name]['training_time'] for name in model_names]\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "bars = axes[0, 0].bar(model_names, r2_scores)\n",
    "axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "axes[0, 0].set_title('Model R¬≤ Score Comparison')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Color bars based on performance\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    if score >= 0.8:\n",
    "        bar.set_color('green')\n",
    "    elif score >= 0.6:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 1].bar(model_names, mae_scores)\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error ($)')\n",
    "axes[0, 1].set_title('Model MAE Comparison')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 0].bar(model_names, training_times)\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 0].set_title('Model Training Time Comparison')\n",
    "plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Prediction accuracy scatter plot (best model)\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['test_r2'])\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "axes[1, 1].scatter(y_test, best_predictions, alpha=0.6)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Price ($)')\n",
    "axes[1, 1].set_ylabel('Predicted Price ($)')\n",
    "axes[1, 1].set_title(f'Prediction Accuracy - {best_model_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"R¬≤ Score: {model_results[best_model_name]['test_r2']:.3f}\")\n",
    "print(f\"MAE: ${model_results[best_model_name]['test_mae']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Explainability {#explainability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model for explainability analysis\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "# Create feature names (simplified for demo)\n",
    "feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create model explainer\n",
    "print(f\"Creating model explainer for {best_model_name}...\")\n",
    "explainer = ModelExplainer(best_model, feature_names)\n",
    "\n",
    "# Generate comprehensive model explanation\n",
    "print(\"Generating model explanation...\")\n",
    "explanation = explainer.explain_model(\n",
    "    X_train, y_train, \n",
    "    importance_methods=['builtin', 'permutation']\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Explanation Summary:\")\n",
    "print(f\"  Model: {explanation.model_name}\")\n",
    "print(f\"  Features analyzed: {len(explanation.feature_importances)}\")\n",
    "\n",
    "# Performance summary\n",
    "performance = explanation.model_performance\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  R¬≤ Score: {performance['r2']:.3f}\")\n",
    "print(f\"  MAE: ${performance['mae']:.2f}\")\n",
    "print(f\"  RMSE: ${performance['rmse']:.2f}\")\n",
    "print(f\"  Mean Prediction: ${performance['mean_prediction']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "builtin_features = [f for f in explanation.feature_importances if f.importance_type == 'builtin']\n",
    "perm_features = [f for f in explanation.feature_importances if f.importance_type == 'permutation']\n",
    "\n",
    "print(f\"Feature Importance Analysis:\")\n",
    "print(f\"  Built-in importance features: {len(builtin_features)}\")\n",
    "print(f\"  Permutation importance features: {len(perm_features)}\")\n",
    "\n",
    "# Top features by built-in importance\n",
    "if builtin_features:\n",
    "    top_builtin = sorted(builtin_features, key=lambda x: x.importance_score, reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 Features (Built-in Importance):\")\n",
    "    for i, feature in enumerate(top_builtin, 1):\n",
    "        print(f\"  {i:2d}. {feature.feature_name}: {feature.importance_score:.4f}\")\n",
    "\n",
    "# Top features by permutation importance\n",
    "if perm_features:\n",
    "    top_perm = sorted(perm_features, key=lambda x: x.importance_score, reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 Features (Permutation Importance):\")\n",
    "    for i, feature in enumerate(top_perm, 1):\n",
    "        ci_text = \"\"\n",
    "        if feature.confidence_interval:\n",
    "            ci_width = feature.confidence_interval[1] - feature.confidence_interval[0]\n",
    "            ci_text = f\" (¬±{ci_width/2:.4f})\"\n",
    "        stability_text = \"\"\n",
    "        if feature.stability_score:\n",
    "            stability_text = f\" [stability: {feature.stability_score:.2f}]\"\n",
    "        print(f\"  {i:2d}. {feature.feature_name}: {feature.importance_score:.4f}{ci_text}{stability_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Importance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature importance dashboard\n",
    "dashboard_fig = create_feature_importance_dashboard(explanation)\n",
    "plt.show()\n",
    "\n",
    "# Display global explanation\n",
    "print(f\"\\nGlobal Model Explanation:\")\n",
    "print(\"=\" * 50)\n",
    "print(explanation.global_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain individual predictions for interesting samples\n",
    "test_indices = [\n",
    "    np.argmax(y_test),  # Most expensive test\n",
    "    np.argmin(y_test),  # Least expensive test\n",
    "    len(y_test) // 2    # Median test\n",
    "]\n",
    "\n",
    "sample_descriptions = [\"Most Expensive\", \"Least Expensive\", \"Median Price\"]\n",
    "\n",
    "print(f\"\\nIndividual Prediction Explanations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    description = sample_descriptions[i]\n",
    "    \n",
    "    # Get prediction explanation\n",
    "    pred_explanation = explainer.explain_prediction(\n",
    "        X_test[idx], \n",
    "        sample_id=f\"sample_{idx}\",\n",
    "        actual_value=y_test.iloc[idx]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{description} Sample (Index: {idx}):\")\n",
    "    print(f\"  Predicted: ${pred_explanation.prediction:.2f}\")\n",
    "    print(f\"  Actual: ${pred_explanation.actual_value:.2f}\")\n",
    "    \n",
    "    error = abs(pred_explanation.prediction - pred_explanation.actual_value)\n",
    "    error_pct = error / pred_explanation.actual_value * 100\n",
    "    print(f\"  Error: ${error:.2f} ({error_pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"  Top Contributing Features:\")\n",
    "    for j, (feature, contribution) in enumerate(pred_explanation.top_contributing_features[:5], 1):\n",
    "        direction = \"‚Üë\" if contribution > 0 else \"‚Üì\"\n",
    "        print(f\"    {j}. {feature}: {direction} ${abs(contribution):.2f}\")\n",
    "    \n",
    "    if pred_explanation.explanation_text:\n",
    "        print(f\"  Explanation:\")\n",
    "        # Split explanation into lines for better formatting\n",
    "        lines = pred_explanation.explanation_text.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                print(f\"    {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Cost Estimator {#integration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create original cost table from our base data\n",
    "cost_table = {}\n",
    "for test_name, cpt_code, base_price, category in base_tests[:10]:  # Use subset for demo\n",
    "    cost_table[test_name] = CptCost(\n",
    "        cpt_code=cpt_code,\n",
    "        price=base_price,\n",
    "        category=category\n",
    "    )\n",
    "\n",
    "print(f\"Created cost table with {len(cost_table)} entries\")\n",
    "\n",
    "# Create ML-enhanced cost estimator\n",
    "ml_config = MLModelConfig(\n",
    "    model_type=\"random_forest\",\n",
    "    model_params={\"n_estimators\": 100, \"random_state\": 42},\n",
    "    performance_threshold=0.7,\n",
    "    enable_model_selection=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCreating ML-enhanced cost estimator...\")\n",
    "ml_estimator = create_ml_enhanced_cost_estimator(\n",
    "    cost_table=cost_table,\n",
    "    training_data=df_train,\n",
    "    ml_config=ml_config\n",
    ")\n",
    "\n",
    "# Display model performance\n",
    "performance = ml_estimator.get_model_performance()\n",
    "if performance:\n",
    "    print(f\"\\nML Model Performance:\")\n",
    "    print(f\"  Model: {performance['model_name']}\")\n",
    "    print(f\"  Performance Grade: {ml_estimator.model_performance.performance_grade}\")\n",
    "    print(f\"  R¬≤ Score: {performance['r2_score']:.3f}\")\n",
    "    print(f\"  MAE: ${performance['mean_absolute_error']:.2f}\")\n",
    "    print(f\"  Training Samples: {performance['training_samples']:,}\")\n",
    "    print(f\"  Features: {performance['feature_count']}\")\n",
    "    print(f\"  Training Time: {performance['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Integration with Different Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different scenarios\n",
    "test_scenarios = [\n",
    "    # Known tests (should use lookup)\n",
    "    ('complete blood count', 'Known test - should use lookup'),\n",
    "    ('chest x-ray 2 views', 'Known test - should use lookup'),\n",
    "    \n",
    "    # Similar to known tests (should use ML with high confidence)\n",
    "    ('complete blood count with differential', 'Similar to known test'),\n",
    "    ('chest x-ray single view', 'Similar to known test'),\n",
    "    ('basic metabolic panel comprehensive', 'Similar to known test'),\n",
    "    \n",
    "    # Completely unknown tests (should use ML or fallback)\n",
    "    ('advanced cardiac biomarkers panel', 'Unknown test - cardiac'),\n",
    "    ('molecular genetic testing brca1', 'Unknown test - genetic'),\n",
    "    ('pet scan whole body with contrast', 'Unknown test - imaging'),\n",
    "    ('specialized neurological function test', 'Unknown test - neuro')\n",
    "]\n",
    "\n",
    "print(f\"Testing Cost Estimation Integration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Test Name':<40} {'Price':<10} {'Category':<12} {'Source':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for test_name, description in test_scenarios:\n",
    "    try:\n",
    "        # Get cost estimate\n",
    "        cost, category = ml_estimator.estimate(test_name)\n",
    "        \n",
    "        # Determine source\n",
    "        source = \"Lookup\"\n",
    "        try:\n",
    "            # Check if it's in the original cost table\n",
    "            ml_estimator.lookup_cost(test_name)\n",
    "        except KeyError:\n",
    "            # Not in lookup table\n",
    "            if ml_estimator.ml_model:\n",
    "                try:\n",
    "                    ml_pred, confidence = ml_estimator.predict_ml_cost(test_name)\n",
    "                    if confidence >= ml_estimator.confidence_threshold:\n",
    "                        source = f\"ML ({confidence:.2f})\"\n",
    "                    else:\n",
    "                        source = \"ML (low conf)\"\n",
    "                except:\n",
    "                    source = \"Fallback\"\n",
    "            else:\n",
    "                source = \"Fallback\"\n",
    "        \n",
    "        # Display result\n",
    "        print(f\"{test_name:<40} ${cost:<9.2f} {category:<12} {source:<15}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{test_name:<40} {'ERROR':<10} {'N/A':<12} {str(e)[:15]:<15}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Source Legend: Lookup=Direct table lookup, ML=Machine Learning, Fallback=LLM or average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanations for some predictions\n",
    "explanation_tests = [\n",
    "    'advanced cardiac biomarkers panel',\n",
    "    'pet scan whole body with contrast',\n",
    "    'basic metabolic panel comprehensive'\n",
    "]\n",
    "\n",
    "print(f\"\\nPrediction Explanations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_name in explanation_tests:\n",
    "    explanation = ml_estimator.explain_prediction(test_name)\n",
    "    \n",
    "    if 'error' in explanation:\n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(f\"  Error: {explanation['error']}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{test_name}:\")\n",
    "    print(f\"  Predicted Cost: ${explanation['prediction']:.2f}\")\n",
    "    \n",
    "    if 'top_features' in explanation:\n",
    "        print(f\"  Top Contributing Features:\")\n",
    "        for i, (feature, contribution) in enumerate(explanation['top_features'][:3], 1):\n",
    "            direction = \"increases\" if contribution > 0 else \"decreases\" \n",
    "            print(f\"    {i}. {feature}: {direction} cost by ${abs(contribution):.2f}\")\n",
    "    \n",
    "    if 'explanation_text' in explanation and explanation['explanation_text']:\n",
    "        print(f\"  Explanation: {explanation['explanation_text'].split('.')[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance analysis\n",
    "print(f\"Comprehensive Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Feature engineering performance\n",
    "feature_processing_rate = len(df_train) / preprocessing_time\n",
    "print(f\"\\nFeature Engineering Performance:\")\n",
    "print(f\"  Total samples processed: {len(df_train):,}\")\n",
    "print(f\"  Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  Processing rate: {feature_processing_rate:.0f} samples/second\")\n",
    "print(f\"  Features generated: {X_processed.shape[1]}\")\n",
    "print(f\"  Feature generation rate: {X_processed.shape[1]/preprocessing_time:.1f} features/second\")\n",
    "\n",
    "# Model performance by category\n",
    "print(f\"\\nModel Performance by Test Category:\")\n",
    "print(f\"{'Category':<15} {'Samples':<8} {'R¬≤ Score':<10} {'MAE':<10} {'MAPE':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Split test data by category for analysis\n",
    "test_df = df_train.iloc[X_test.index] if hasattr(X_test, 'index') else df_train[-len(y_test):]\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "for category in df_train['category'].unique():\n",
    "    # Get indices for this category in test set\n",
    "    if hasattr(test_df, 'reset_index'):\n",
    "        category_mask = test_df.reset_index()['category'] == category\n",
    "    else:\n",
    "        category_mask = test_df['category'] == category\n",
    "    \n",
    "    if category_mask.sum() > 0:\n",
    "        y_cat_true = y_test[category_mask]\n",
    "        y_cat_pred = best_predictions[category_mask]\n",
    "        \n",
    "        if len(y_cat_true) > 1:\n",
    "            cat_r2 = r2_score(y_cat_true, y_cat_pred)\n",
    "            cat_mae = mean_absolute_error(y_cat_true, y_cat_pred)\n",
    "            cat_mape = np.mean(np.abs((y_cat_true - y_cat_pred) / np.maximum(y_cat_true, 1e-8))) * 100\n",
    "            \n",
    "            print(f\"{category:<15} {len(y_cat_true):<8} {cat_r2:<10.3f} ${cat_mae:<9.2f} {cat_mape:<9.1f}%\")\n",
    "\n",
    "# Overall system performance\n",
    "print(f\"\\nOverall System Performance:\")\n",
    "total_pipeline_time = preprocessing_time + model_results[best_model_name]['training_time']\n",
    "print(f\"  End-to-end training time: {total_pipeline_time:.2f} seconds\")\n",
    "print(f\"  Prediction latency: {model_results[best_model_name]['prediction_time']/len(y_test)*1000:.1f} ms/sample\")\n",
    "print(f\"  Memory footprint: ~{X_processed.nbytes / 1024 / 1024:.1f} MB (features only)\")\n",
    "print(f\"  Model accuracy: {model_results[best_model_name]['test_r2']:.1%}\")\n",
    "print(f\"  Model precision: ${model_results[best_model_name]['test_mae']:.2f} MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different data sizes\n",
    "print(f\"\\nPerformance Scaling Analysis:\")\n",
    "print(f\"{'Data Size':<12} {'Processing Time':<16} {'Features':<10} {'Rate (samples/s)':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Test with different sample sizes\n",
    "test_sizes = [100, 500, 1000, len(df_train)]\n",
    "\n",
    "for size in test_sizes:\n",
    "    if size <= len(df_train):\n",
    "        # Sample data\n",
    "        sample_data = df_train.sample(n=size, random_state=42)\n",
    "        sample_input = sample_data[['test_name', 'cpt_code', 'price']]\n",
    "        \n",
    "        # Time the processing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create fresh pipeline for fair comparison\n",
    "        sample_pipeline = create_default_preprocessing_pipeline()\n",
    "        sample_features = sample_pipeline.fit_transform(sample_input)\n",
    "        \n",
    "        process_time = time.time() - start_time\n",
    "        rate = size / process_time\n",
    "        \n",
    "        print(f\"{size:<12,} {process_time:<16.3f} {sample_features.shape[1]:<10} {rate:<15.0f}\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nMemory Usage Estimation:\")\n",
    "feature_memory = X_processed.nbytes / 1024 / 1024\n",
    "print(f\"  Feature matrix: {feature_memory:.1f} MB\")\n",
    "print(f\"  Per sample: {feature_memory / len(X_processed) * 1024:.1f} KB\")\n",
    "print(f\"  Estimated for 100K samples: {feature_memory / len(X_processed) * 100000:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Deployment Considerations {#deployment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for production deployment\n",
    "model_save_path = \"../models/ml_cost_estimator_demo.pkl\"\n",
    "\n",
    "print(f\"Production Deployment Checklist:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    import os\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    ml_estimator.save_ml_model(model_save_path)\n",
    "    print(f\"‚úÖ Model saved to {model_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model save failed: {e}\")\n",
    "\n",
    "# Model validation checklist\n",
    "print(f\"\\nüìã Model Validation Checklist:\")\n",
    "validation_checks = [\n",
    "    (\"Model R¬≤ Score > 0.7\", model_results[best_model_name]['test_r2'] > 0.7),\n",
    "    (\"Model MAE < $50\", model_results[best_model_name]['test_mae'] < 50),\n",
    "    (\"Training completed successfully\", ml_estimator.model_performance is not None),\n",
    "    (\"Feature engineering working\", X_processed.shape[1] > 0),\n",
    "    (\"No major validation failures\", len(summary.get('major_issues', [])) == 0),\n",
    "    (\"Prediction explanations available\", ml_estimator.model_explainer is not None)\n",
    "]\n",
    "\n",
    "for check, passed in validation_checks:\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"  {status} {check}\")\n",
    "\n",
    "# Performance requirements\n",
    "print(f\"\\n‚ö° Performance Requirements:\")\n",
    "perf_requirements = [\n",
    "    (\"Prediction latency < 100ms\", model_results[best_model_name]['prediction_time']/len(y_test) < 0.1),\n",
    "    (\"Training time < 60s\", model_results[best_model_name]['training_time'] < 60),\n",
    "    (\"Memory usage reasonable\", feature_memory < 500),  # Less than 500MB\n",
    "    (\"Feature processing < 10s\", preprocessing_time < 10)\n",
    "]\n",
    "\n",
    "for requirement, met in perf_requirements:\n",
    "    status = \"‚úÖ\" if met else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status} {requirement}\")\n",
    "\n",
    "# Deployment recommendations\n",
    "print(f\"\\nüöÄ Deployment Recommendations:\")\n",
    "print(f\"  ‚Ä¢ Enable feature caching for frequently requested tests\")\n",
    "print(f\"  ‚Ä¢ Implement prediction confidence thresholds (current: {ml_estimator.confidence_threshold})\")\n",
    "print(f\"  ‚Ä¢ Set up model performance monitoring\")\n",
    "print(f\"  ‚Ä¢ Plan for regular model retraining (monthly/quarterly)\")\n",
    "print(f\"  ‚Ä¢ Implement A/B testing for model updates\")\n",
    "print(f\"  ‚Ä¢ Set up data quality monitoring for input features\")\n",
    "print(f\"  ‚Ä¢ Configure fallback mechanisms for ML failures\")\n",
    "\n",
    "# Production integration example\n",
    "print(f\"\\nüîß Production Integration Example:\")\n",
    "print(f\"\"\"```python\n",
    "# Load trained model in production\n",
    "from sdb.ml_cost_estimator import MLCostEstimator\n",
    "\n",
    "# Load model\n",
    "estimator = MLCostEstimator(cost_table)\n",
    "estimator.load_ml_model('{model_save_path}')\n",
    "\n",
    "# Make predictions with confidence checking\n",
    "cost, category = estimator.estimate('new test name')\n",
    "explanation = estimator.explain_prediction('new test name')\n",
    "\n",
    "# Monitor performance\n",
    "performance = estimator.get_model_performance()\n",
    "if performance['r2_score'] < 0.7:\n",
    "    # Trigger retraining alert\n",
    "    pass\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ML FEATURE ENGINEERING SYSTEM - DEMO SUMMARY\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  ‚Ä¢ Training samples: {len(df_train):,}\")\n",
    "print(f\"  ‚Ä¢ Unique tests: {df_train['test_name'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Price range: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "print(f\"  ‚Ä¢ Test categories: {df_train['category'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüîß Feature Engineering Results:\")\n",
    "print(f\"  ‚Ä¢ Features generated: {X_processed.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Processing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Processing rate: {len(df_train)/preprocessing_time:.0f} samples/second\")\n",
    "print(f\"  ‚Ä¢ Feature quality: {summary.get('avg_quality_score', 0):.3f} average score\")\n",
    "\n",
    "print(f\"\\nü§ñ Model Performance:\")\n",
    "print(f\"  ‚Ä¢ Best model: {best_model_name}\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {model_results[best_model_name]['test_r2']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Mean Absolute Error: ${model_results[best_model_name]['test_mae']:.2f}\")\n",
    "print(f\"  ‚Ä¢ Mean Absolute Percentage Error: {model_results[best_model_name]['test_mape']:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Performance Grade: {MLCostEstimator(cost_table).model_performance.performance_grade if MLCostEstimator(cost_table).model_performance else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nüéØ System Capabilities:\")\n",
    "print(f\"  ‚úÖ Comprehensive feature extraction from CPT codes and test names\")\n",
    "print(f\"  ‚úÖ Advanced data preprocessing with quality validation\")\n",
    "print(f\"  ‚úÖ Multiple ML model support with automated selection\")\n",
    "print(f\"  ‚úÖ Feature importance analysis and model explainability\")\n",
    "print(f\"  ‚úÖ Seamless integration with existing cost estimator\")\n",
    "print(f\"  ‚úÖ Production-ready deployment with monitoring\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Production:\")\n",
    "all_checks_passed = all([check[1] for check in validation_checks])\n",
    "if all_checks_passed:\n",
    "    print(f\"  ‚úÖ All validation checks passed\")\n",
    "    print(f\"  ‚úÖ Performance requirements met\")\n",
    "    print(f\"  ‚úÖ Model saved and ready for deployment\")\n",
    "    print(f\"  ‚úÖ System is production-ready!\")\n",
    "else:\n",
    "    failed_checks = [check[0] for check in validation_checks if not check[1]]\n",
    "    print(f\"  ‚ö†Ô∏è  Some validation checks failed: {', '.join(failed_checks)}\")\n",
    "    print(f\"  üìù Review and address issues before production deployment\")\n",
    "\n",
    "print(f\"\\nüìà Next Steps:\")\n",
    "print(f\"  1. Deploy model to production environment\")\n",
    "print(f\"  2. Implement monitoring and alerting\")\n",
    "print(f\"  3. Set up automated retraining pipeline\")\n",
    "print(f\"  4. Conduct A/B testing with current system\")\n",
    "print(f\"  5. Monitor performance and gather feedback\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Demo completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "For more information about the ML Feature Engineering System:\n",
    "\n",
    "- **Documentation**: See `docs/ml_feature_engineering_guide.md` for comprehensive usage guide\n",
    "- **API Reference**: Detailed API documentation in the guide\n",
    "- **Performance Benchmarks**: Complete benchmarking results in the documentation\n",
    "- **Troubleshooting**: Common issues and solutions in the guide\n",
    "- **Best Practices**: Production deployment recommendations\n",
    "\n",
    "### Key Components Used:\n",
    "\n",
    "1. **`sdb.ml_feature_engineering`**: Core feature engineering pipeline\n",
    "2. **`sdb.ml_preprocessing`**: Data preprocessing and validation\n",
    "3. **`sdb.ml_cost_estimator`**: ML-enhanced cost estimation\n",
    "4. **`sdb.ml_validation`**: Comprehensive testing framework\n",
    "5. **`sdb.ml_explainability`**: Model interpretation and explanations\n",
    "6. **`sdb.ml_feature_store`**: Feature storage and caching system\n",
    "\n",
    "The system is now ready for production deployment with robust feature engineering, model training, and explainability capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}